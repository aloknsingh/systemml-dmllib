# THIS SCRIPT SOLVES TOPIC MODELLING PROBLEM USING THE Online Variational Bayes Latent Dirichlet Allocation( Online_VB_LDA)
#
# ASSUMPTIONS:
#   Individual docs are represented as the bag of words models.
#   The input already been pre-processed i.e tokenization, stop word removable, stemming etc are done
#       one of the ways we can find the vocabulary for the corpus is to use the tfi-df
#   The each line in input corpus contains termVector
# INPUT PARAMETERS:
# --------------------------------------------------------------------------------------------
# NAME     TYPE     DEFAULT  MEANING
# --------------------------------------------------------------------------------------------
# termVecs String    ---     Location (on HDFS) to read document term vector.each line 
#                                represents the doc and it's content is the id of word from doc in vocab                             
# termCnts String    ---     Location (on HDFS) to read term counts. Each line represents the doc and it's content 
#                                is the count of the word whose id is in the termVecs
# maxd     Int       ---     Only for the online LDA, the estimated maximum documents. (Note this needn't be exact)
# K        Int       ---     Number of topics in the model
# W        Int       ---     Number of words in the vocabulary
# isonline Int        1      Whether we want to run the online or the batch variational bayes LDA
# alpha    Double    1/K     Dirichlet Hyperparameter for prior on doc' topic weight vectors theta
# eta      Double    1/K     Dirichlet Hyperparameter for prior on topics word weight (distribution) vector, beta
# tau0     Int       1024    A (positive) learning parameter that downweights early iterations 
# kappa    double    0.7     Learning rate: exponential decay rate---should be between
#                                (0.5, 1.0] to guarantee asymptotic convergence
# ofmt     String    "text"  Matrix file output format, such as `text`,`mm`, or `csv` 
# Log      String    " "     Location to store output and variables for monitoring and debugging purposes
# 

# --------------------------------------------------------------------------------------------
# OUTPUT: Matrix of KXW matrix of lambda which is the  parameters 
#
#
# HOW TO INVOKE THIS SCRIPT - EXAMPLE:
# hadoop jar SystemML.jar -f CsplineCG.dml -nvargs X=INPUT_DIR/X Y=INPUT_DIR/Y K=OUTPUT_DIR/K O=OUTPUT_DIR/Out
# tol=0.001 maxi=100 fmt=csv Log=OUTPUT_DIR/log


#Arguments:
#        K: Number of topics
#        vocab: A set of words to recognize. When analyzing documents, any word
#           not in this set will be ignored.
#        D: Total number of documents in the population. For a fixed corpus,
#           this is the size of the corpus. In the truly online setting, this
#           can be an estimate of the maximum number of documents that
#           could ever be seen.
#        alpha: Hyperparameter for prior on weight vectors theta
#        eta: Hyperparameter for prior on topics beta
#        tau0: A (positive) learning parameter that downweights early iterations
#        kappa: Learning rate: exponential decay rate---should be between
#             (0.5, 1.0] to guarantee asymptotic convergence.
#
#        Note that if you pass the same set of D documents in every time and
#        set kappa=0 this class can also be used to do batch VB.
#
# Initialize the algorithm with alpha=1/K, eta=1/K, tau_0=1024, kappa=0.7
ignore=main()
#ignore=test_online_VB_LDA()

main = function() return() {
 print("Starting Topic modelling...")    
 D = 1 # total number of documents #@TODO take it from user options
 K = $K
 W = $W
 isonline = ifdef($isonline, 1)
 alpha = ifdef($alpha, 1.0/K) 
 eta = ifdef($eta, 1.0/K)  # usually 1/K
 tau0 = ifdef($tau0, 1024)
 tau0 = tau0 + 1 # case when the user passes in 0
 kappa = ifdef($kappa, 0.7) # must be [0.5, 1) 
 Log = ifdef($Log, " ")

 batchDocsIds = read($termVecs)
 batchDocsCts = read($termCnts)

 updatect = 0


 # for testing # ALOK_DEBUG
 # batchDocsIds = matrix("1 2 3 4 5", rows=1, cols=5)
 # batchDocsCts = matrix("6 2 2 3 4", rows=1, cols=5)
 [gamma, updatedLambda] = online_VB_LDA(batchDocsIds, batchDocsCts, W, K, D, alpha, eta, tau0, kappa, updatect)

 write(updatedLambda, "updatedLambda.mtx")
 print("Ending topic modelling")
}
#for debug
#ignore=test_online_VB_LDA()

test_online_VB_LDA = function(
) return () {
  print("Starting Topic modelling...")    
W = 5 # num of words in the vocab
K = 2 # num of topics
#D = 1 # total number of documents
D = 2 # total number of documents

isonline = 1
alpha = 1.0/K
eta = 1.0/K  # usually 1/K
tau0 = 1024
tau0 = tau0 + 1 # case when the user passes in 0
kappa =  0.7 # must be [0.5, 1) 
Log = ifdef($Log, " ")

updatect = 0


# for testing # ALOK_DEBUG
#batchDocsIds = matrix("1 2 3 4 5", rows=1, cols=5)
#batchDocsCts = matrix("6 2 2 3 4", rows=1, cols=5)

batchDocsIds = matrix("1 2 3 4 5 1 2 3 4 5", rows=2, cols=5)
batchDocsCts = matrix("6 2 2 3 4 2 3 6 4 3", rows=2, cols=5)

batchDocsIds = matrix("1 2 3 4 5 1 2 3 4 5", rows=2, cols=5)
batchDocsCts = matrix("6 2 2 3 4 0 0 6 0 2", rows=2, cols=5)

[gamma, updatedLambda] = online_VB_LDA(batchDocsIds, batchDocsCts, W, K, D, alpha, eta, tau0, kappa, updatect)

ignore=printm("M_updatedLambda", updatedLambda)
print("Ending topic modelling")

}
#@NOTE: in future, all the extra arguement will be reduced as we will have struct/class in system dml
online_VB_LDA = function(
  matrix[double] batchDocsWordIds, # the batchDocsXdocsId matrix
  matrix[double] batchDocsWordCts, # thebatchDocsXwordCnts matrix
  int W, # number of words in the vocab
  int K, # number of topics
  double corpusSize, # total number of documents
  double alpha, # dir parameter for topic distribution per doc of true distribution. #usually  = 1.0/K 
  double eta, # dir parameter word distribution per topic. for the  = 1.0/K  # usually 1/K
  double tau0, # A (positive) learning parameter that downweights early iterations 
  double kappa, # Learing rate we choose 0.7 # must be [0.5, 1) 
  int updatect
) return (matrix[double] gamma, matrix[double] updatedLambda) {

  if (nrow(batchDocsWordIds) != nrow(batchDocsWordCts)) {
    stop("batchDocsWordIds and batchDocsWordCts much match")
  }
  batchDocsSize = nrow(batchDocsWordIds) # total number of documents in this batch

  gamma = matrix(0, rows=batchDocsSize, cols=K) # 
  sstats = matrix(0, rows=K, cols=W) # sufficient stats i.e lambda
 
  #Init the variational distribution q(beta|lambda) # get or read the Lambda
  Lambda = get_random_gamma(K, W) #TODO init by random func 
  #ALOK DEBUG
  Lambda = matrix("0.96307558 1.05341124 0.91342724 0.9911178 1.09593632 1.1080743 0.90388002 0.99404929 0.83463754 0.97659028", rows=2, cols=5)

  
  [gamma, updatedLambda] = batch_variational_sufficient_stats(batchDocsWordIds, batchDocsWordCts, Lambda, W, K, corpusSize, 
                                                   alpha, eta, tau0, kappa, updatect)

  

  updatect = updatect + 1
  
}



#@NOTE: in future, all the extra arguement will be reduced as we will have struct/class in system dml
batch_variational_sufficient_stats = function(
  matrix[double] batchDocsWordIds, # the batchDocsXdocsId matrix
  matrix[double] batchDocsWordCts, # thebatchDocsXwordCnts matrix
  matrix[double] Lambda,
  int W, # number of words in the vocab
  int K, # number of topics
  double corpusSize, # total number of documents
  double alpha, # dir parameter for topic distribution per doc of true distribution. #usually  = 1.0/K 
  double eta, # dir parameter word distribution per topic. for the  = 1.0/K  # usually 1/K
  double tau0, # A (positive) learning parameter that downweights early iterations 
  double kappa, # Learing rate we choose 0.7 # must be [0.5, 1) 
  double updatect
) return (matrix[double] gamma, matrix[double] updatedLambda) {

  if (nrow(batchDocsWordIds) != nrow(batchDocsWordCts)) {
    stop("batchDocsWordIds and batchDocsWordCts much match")
  }
  batchDocsSize = nrow(batchDocsWordIds) # total number of documents in this batch

  
  [Elogbeta, expElogbeta] = dirichlet_expected_loglikelihood(Lambda)
  

  
# begin variational bayes
  

  batchDocsSStats = matrix(0.0, rows=K, cols=W)
  #init the variation distribution q(theta|gamma)
  batchD = 1
  gamma = get_random_gamma(batchDocsSize, K)
  #gamma =  matrix("0.93880941 0.8567902", rows=1, cols=K) #ALOK_DEBUG
  gamma =  matrix("0.93880941 0.8567902 1.11048865 0.97495979", rows=2, cols=K) #ALOK_DEBUG

  #ignore=printm("M_gamma", gamma)
  # do all the E steps in parallel par
  for (docId in 1:batchDocsSize) {
    #ids = wordids[d,]
    #cts = wordcts[d,]
    #note that we work with column vectors so we need extra tranpose. Hopefully compiler optimizer will take care of this
    docWordIds = t(batchDocsWordIds[docId,])
    docWordCts = t(batchDocsWordCts[docId,])
    gammad = gamma[docId, ]
    
    [docGamma, docSStatsCompact] = doc_variational_sufficient_stats(docWordIds, docWordCts, gammad, expElogbeta, alpha, K)
                         
    #gammad = matrix(0.0, rows=1, cols=K) #ALOK_DEBUG
    #sstatsd = matrix(0.0, rows=K, cols=W) #ALOK_DEBUG
    gamma[docId,] = docGamma
    # since sstatsd is not aligned wrt the KXW matrix aligned it
    docSStats = matrix(0.0, rows=K, cols=W) 
    #docWordIdsSStats = mcols(docSStats, docWordIds) # only columns corresponding to the words in docId
    #docWordIdsSStats = docWordIdsSStats + docSStatsCompact #
    #docSStats = msetcols(docSStats, docSStatsCompact, docWordIds)
    docSStats = msetcols(docSStats, docSStatsCompact, docWordIds)
    batchDocsSStats = batchDocsSStats + docSStats
    ignore = printm("M_batchDocsSStats_1", batchDocsSStats)
  }

  # M steps
  # Maximize the resulting lower bound
  batchDocsSStats = batchDocsSStats * expElogbeta
  ignore = printm("M_batchDocsSStats_2", batchDocsSStats)

  # rhot = [0,1]  and says how much weight we want to give to the mini batch
  # note 
  #  - 1) that at it is exponential in nature to the updatect which is the number of batches we process. 
  #       so it is giving more weight to the latest batch. 
  #  - 2) when kappa == 0, then it reduces to 1 meaning , we give 100% weight to the last batch and 0 to the previous stats and it is not likely to converge
  #  - 3) when kappa == 1, then it reduces to simple weighted average
  #  - 3) usually kappa is (0.5, 1) and we choose 0.7, it that case it will converge
  rhot = (tau0  + updatect)^-kappa

  # Lambda batch is the controbution to the direchlet variational hyper parameter lambda by this batch of docs
  LambdaBatch = eta + batchDocsSStats*corpusSize/batchDocsSize
  ignore=printm("M_LambdaBatch", LambdaBatch)
  
  # calculate the new weighted direchlet variational hyper parameter lambda on the older lambda and the batch lambda
  # since 0 < rhot < 1, we can view the newLambda as the comvex combination of oldLambda and batchLambda for each batch.
  # geometrically, we can view this as refining the convex hull of Lambda in every update. Since rhot is more closer to 0
  # each new batch contribition is comparatively small.
  updatedLambda = Lambda * (1-rhot) + LambdaBatch*rhot
  ignore=printm("M_updatedLambda", updatedLambda)

  
  
}


#@description: find the topic variational param i.e gamma for the doc, which is represented by termVector and 
#              term counts.
#@output: the variational parameters {gamma, sstats}
#          sstats = function(constant(expElogbeta), termCntVector, phi)
#          in docs and others' implementation, they return {gamma, phi}              
doc_variational_sufficient_stats = function(
  matrix[double] termVector,
  matrix[double] termCounts,
  matrix[double] gammad,
  matrix[double] expElogbeta,
  double alpha,
  int K # size of the topics
) return (matrix[double] gammad, matrix[double] sstatsd) {
    
  # begin variational bayes
  ids = termVector
  cts = termCounts
  
  [Elogthetad, expElogthetad] = dirichlet_expected_loglikelihood(gammad)
  #ignore = printm("M_Elogthetad", Elogthetad); ignore = printm("M_expElogthetad", expElogthetad)
  ignore = printm("M_ids", ids)

  expElogbetad = mcols(expElogbeta, ids)
  ignore = printm("M_expElogbetad", expElogbetad )
  phiNorm = mdot(t(expElogthetad), expElogbetad)
  phiNorm = phiNorm + 1e-100 # to account for the zero dot product
  ignore = printm("M_phiNorm", phiNorm)

  # do the EM steps. Iterate between gamma and phi until convergence
  meanchange = 0
  #@TODO: do we need to pass from user? for now we will not and after discussion we will decide.
  meanchangethresh = 1e-03 #try 1e-03
  #@TODO: do we need to pass from user? for now we will not and after discussion we will decide.
  e_max_it = 100 # should be 100

  
  meanchange = 1.0  
  e_it = 1
  while (meanchange > meanchangethresh & e_it <= e_max_it) { 
    print ("Gamma Iteration Id = " + e_it)
    # store the last result to calculate the mean change
    lastgammad = gammad
   
    # BEGIN DEBUG CODE
    # t1=t(cts)/phiNorm
    # ignore=printm("M_t1", t1)
    # t2=mdot(t(t1), t(expElogbetad))
    # ignore=printm("M_t2", t2)
    # t3=expElogthetad*t2
    # ignore=printm("M_t3", t3)
    # gammad = alpha + t3
    # END DEBUG CODE

    #we represent phi implicitly to save memory and time. 
    tmp_gammad = mdot(t(t(cts)/phiNorm), t(expElogbetad))
    gammad = alpha + expElogthetad*(tmp_gammad)
    #ignore=printm("M gammad at it ", gammad)

    [Elogthetad, expElogthetad] = dirichlet_expected_loglikelihood(gammad)
    #ignore=printm("M_etd1", Elogthetad);ignore=printm("M_eetd1", expElogthetad)
    phiNorm=mdot(t(expElogthetad), expElogbetad)
    #ignore = printm("M_pn1", phiNorm)

    meanchange=mean(abs(gammad-lastgammad))
    print("meanchange="+meanchange)
  

    e_it = e_it + 1 
  }
  
  #return the sufficient stats for a doc
  gammad = gammad 
  sstatsd = outer(t(expElogthetad), t(cts)/phiNorm, "*")
  ignore=printm("M_doc_variational_sufficient_stats:gammad", gammad)
  ignore=printm("M_doc_variational_sufficient_stats:sstatsd", sstatsd)
#end of variational topic_inference
}


# Assumption, we assume that the vectors are represented as the columns
#ref https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf Appendix A.1
# 
# @input: hyperparam
# @output: ElogDist: E[log(RV)] where RV ~ Dir(hyperparams)
# @output: expElogDist: exponential of ElogDist which is kind of E[RV]
#          because of jensen's inequality exp(E[log(RV)]) <= exp(log(E(RV))) == E(RV)
dirichlet_expected_loglikelihood = function (matrix[double] hyperparams)
return (matrix[double] ELogRV, matrix[double] expELogRV) {
  dig_hyperparams = digamma(hyperparams)
  sum_hyperparams = rowSums(hyperparams)
  dig_sum_hyperparams = digamma(sum_hyperparams)
  ELogRV = dig_hyperparams - dig_sum_hyperparams
  expELogRV = exp(ELogRV)
}

#@description:A convenient wrapper over library gamma random variable. Here we assume the shape and scale for this DML
#@input: rows, cols: shape of the output
#@output: matrix of dim(rows X cols), where each cell is the random gamma sample drawn from shape=100 and scale=1/100
get_random_gamma = function(int rows, int cols) 
return (matrix[double] gamma) {
  # since we are initializing the random gamma
  # the choice of shape and scale parameters are arbitary.
  # also our current implementation of random gamma accepts only the integer shape
  shape = 100 
  scale = 1.0/100.0
  gamma = random_gamma(shape, scale, rows, cols) # size of gamma #TODO initialize randomly
   
}

#include "math/special/DiGamma.dml"
#include "math/random/GammaDistribution.dml"
#include "math/matrix/MatrixOps.dml"
#include "math/functions/*.dml"

