#Arguments:
#        K: Number of topics
#        vocab: A set of words to recognize. When analyzing documents, any word
#           not in this set will be ignored.
#        D: Total number of documents in the population. For a fixed corpus,
#           this is the size of the corpus. In the truly online setting, this
#           can be an estimate of the maximum number of documents that
#           could ever be seen.
#        alpha: Hyperparameter for prior on weight vectors theta
#        eta: Hyperparameter for prior on topics beta
#        tau0: A (positive) learning parameter that downweights early iterations
#        kappa: Learning rate: exponential decay rate---should be between
#             (0.5, 1.0] to guarantee asymptotic convergence.
#
#        Note that if you pass the same set of D documents in every time and
#        set kappa=0 this class can also be used to do batch VB.
#
# Initialize the algorithm with alpha=1/K, eta=1/K, tau_0=1024, kappa=0.7
    
W = 5 # num of words in the vocab
K = 2 # num of topics
V = 5 # vocab in the batch
D = 1 # total number of documents
alpha = 1.0/K 
eta = 1.0/K  # usually 1/K
tau0 = 1024 + 1
kappa = 0.7 # must be [0.5, 1) 
updatect = 0


#Init the variational distribution q(beta|lambda)
Lambda = get_random_gamma(K, W) #TODO init by random func 
#ALOK DEBUG
Lambda = matrix("0.96307558 1.05341124 0.91342724 0.9911178 1.09593632 1.1080743 0.90388002 0.99404929 0.83463754 0.97659028", rows=2, cols=5)

Elogbeta = dirichlet_expectation_logposterior(Lambda)
expElogbeta = exp(Elogbeta)


# rhot is in (0, 1) and says how much weight we want to give to the mini batch
base = tau0 + updatect
nkappa = -kappa
rhot = pows(base, nkappa)

elbo_max_iteration = 1 # should be 100 later
elbo_target = 0.00001
#print("exp(log(10))" + exp(log(10)))
Sstats = matrix(0.0, rows=K, cols=W)

# for testing
ids = matrix("1 2 3 4 5", rows=5, cols=1)
#ids = matrix("0 1 2 3 4", rows=1, cols=5)
cts = matrix("6 2 2 3 4", rows=5, cols=1)

#ids = wordids[d,]
#cts = wordcts[d,]
batchD = 1 # should be given as input


sstats = matrix(0.0, rows=K, cols=W)


meanchange = 1.0  

elbo_it = 1
#init the variation distribution q(theta|gamma)
gamma = get_random_gamma(batchD, K)
docId = 1
gammad = gamma[docId,]
gammad = matrix("0.93880941 0.8567902", rows=1, cols=K) #ALOK debug
ignore=printm("M_gammad", gammad)
Elogthetad = dirichlet_expectation_logposterior(gammad)
expElogthetad = exp(Elogthetad)
ignore = printm("M_Elogthetad", Elogthetad)
#expElogbetad = expElogbeta[,ids]
ignore = printm("M_expElogthetad", expElogthetad)
ignore = printm("M_ids", ids)
expElogbetad = mcols(expElogbeta, ids)
ignore = printm("M_expElogbetad", expElogbetad )
phiNorm = mdot(t(expElogthetad), expElogbetad)
phiNorm = phiNorm + 1e-100 # to account for the zero dot product
ignore = printm("M_phiNorm", phiNorm)

# do the EM steps. Iterate between gamma and phi until convergence
meanchange = 0
meanchangethresh = 1e-03
while (elbo_it <= elbo_max_iteration | meanchange < meanchangethresh) { #try 1e-03
	print ("Iteration Id = " + elbo_it)
	# store the last result to calculate the mean change
	lastgammad = gammad
	# do the EM steps
	t1=t(cts)/phiNorm
	ignore=printm("M_t1", t1)
	t2=mdot(t(t1), t(expElogbetad))
	ignore=printm("M_t2", t2)
	t3=expElogthetad*t2
	ignore=printm("M_t3", t3)
	gammad = alpha + t3
	ignore=printm("M gammad at it ", gammad)
	Elogthetad = dirichlet_expectation_logposterior(gammad)
	ignore=printm("M_etd1", Elogthetad)
    expElogthetad = exp(Elogthetad)
    ignore=printm("M_eetd1", expElogthetad)
    phiNorm=mdot(t(expElogthetad), expElogbetad)
    ignore = printm("M_pn1", phiNorm)

    meanchange=mean(abs(gammad-lastgammad))
    print("meanchange="+meanchange)
	#gammad = alpha + expElogthetad %*%  get_random_gamma(1, K) # size of gamma #TODO initialize randomly
    #mdot(c)
    

	elbo_it = elbo_it + 1 
}
gamma[docId,] = gammad
sstats[, ids] = sstats[,ids] + outer(t(expElogthetad), t(cts)/phiNorm)
/*

sstats = sstats*expElogbeta

LambdaBatch = eta + Stat*D/Dbatch
Lambda = Lambda * (1-rhot) + LambdaBatch*rhot
Elogbeta = dirichlet_expectation_logposterior(Lambda)
expLogbeta = exp(Elogbeta)
updatect = updatect + 1

*/

# Assumption, we assume that the vectors are represented as the columns
#ref https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf Appendix A.1
dirichlet_expectation_logposterior = function (matrix[double] alpha)
return (matrix[double] elogtheta) {
  dig_alpha = digamma(alpha)
  sum_alpha = rowSums(alpha)
  dig_sum_alpha = digamma(sum_alpha)
  elogtheta = dig_alpha - dig_sum_alpha
}

get_random_gamma = function(int rows, int cols) 
return (matrix[double] gamma) {
  # since we are initializing the random gamma
  # the choice of shape and scale parameters are arbitary
  shape = 100 
  scale = 1.0/100.0
  gamma = random_gamma(shape, scale, rows, cols) # size of gamma #TODO initialize randomly
   
}

#include "math/special/DiGamma.dml"
#include "math/random/GammaDistribution.dml"
#include "math/matrix/MatrixOps.dml"
#include "math/functions/*.dml"

